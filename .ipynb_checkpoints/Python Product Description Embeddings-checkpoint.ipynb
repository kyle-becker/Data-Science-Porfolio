{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "torch.manual_seed(1)\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read into product description\n",
    "product_desc = pd.read_csv(\"C:\\\\Users\\\\kyle.becker\\\\Desktop\\\\Product Descriptions.csv\", encoding=\"latin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter df to remove product descriptions with less than 4 words\n",
    "product_hierarchy['length'] = product_hierarchy['PRODUCT_DESC'].str.split(\" \").str.len()\n",
    "product_hierarchy = product_hierarchy[product_hierarchy['length']>4]\n",
    "\n",
    "#remove na columns\n",
    "product_hierarchy = product_hierarchy[product_hierarchy.notnull()]\n",
    "\n",
    "#count rows\n",
    "product_hierarchy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert to list\n",
    "product_desc_full_list = list(product_hierarchy['PRODUCT_DESC'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_words_dict(text):\n",
    "    word_dict = {}\n",
    "    paragraph_dict = {}\n",
    "\n",
    "    #split string\n",
    "    for i in range(len(text)):\n",
    "        split_words = text[i].split(\" \")\n",
    "\n",
    "            #check if word is in dictionary if not add it\n",
    "        for x in range(len(split_words)):\n",
    "            if split_words[x] in word_dict:\n",
    "                x+=1\n",
    "            else:\n",
    "                word_dict[split_words[x]]=len(word_dict)\n",
    "    \n",
    "    \n",
    "    #create paragraph tokens\n",
    "    for i in range(len(text)):\n",
    "        paragraph_dict['paragraph_'+str(i)] = len(word_dict)+i\n",
    "            \n",
    "    return word_dict, paragraph_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create dictionary of words\n",
    "words_dict, paragraph_dict = create_words_dict(product_desc_full_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_words(dictionary,text):\n",
    "    \n",
    "    tokenized_words = []\n",
    "    \n",
    "    for i in range(len(text)):\n",
    "        split_words = text[i].split(\" \")\n",
    "        \n",
    "        for w in range(len(split_words)):\n",
    "            split_words[w] = dictionary.get(split_words[w])\n",
    "\n",
    "        tokenized_words.append(split_words)\n",
    "    \n",
    "    return tokenized_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_target_words(binary_list, paragraph_dict):\n",
    "    target_word_list = []\n",
    "    preprocessed_data = []\n",
    "    for row in range(0,len(binary_list)):\n",
    "        for words in range(2,len(binary_list[row])-2):\n",
    "            context = [binary_list[row][words-2], binary_list[row][words-1], binary_list[row][words+1], binary_list[row][words+2],paragraph_dict['paragraph_'+str(row)]]\n",
    "            target_word = binary_list[row][words]\n",
    "            word_group = context\n",
    "            preprocessed_data.append(word_group)\n",
    "            target_word_list.append(target_word)\n",
    "    return preprocessed_data, target_word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_start = time.time()\n",
    "\n",
    "embedding_dim = 1000\n",
    "vocab_size = len(words_dict)\n",
    "paragraph_size = len(paragraph_dict)\n",
    "context_words_size = 5\n",
    "epoch=1000\n",
    "batch_size = 12000\n",
    "\n",
    "class Word_Embeddings(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, paragraph_size, embedding_dim, context_size):\n",
    "        super(Word_Embeddings, self).__init__()\n",
    "        #create embeddings\n",
    "        self.embedding = nn.Embedding(vocab_size+paragraph_size, embedding_dim)\n",
    "        #pass through neural network hidden layer\n",
    "        self.linear1 = nn.Linear(context_size*embedding_dim ,128)\n",
    "        #pass through softmax layer\n",
    "        self.linear2 = nn.Linear(128, vocab_size+1)\n",
    "        \n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        embedding = self.embedding(inputs)\n",
    "        flatten = embedding.view((embedding.shape[0],-1))\n",
    "        layer_1 = self.linear1(flatten)\n",
    "        relu1 = F.relu(layer_1)\n",
    "        layer_2 = self.linear2(relu1)\n",
    "        log_probs = F.log_softmax(layer_2, dim=1)\n",
    "        return log_probs\n",
    "    \n",
    "losses = []\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "model = Word_Embeddings(vocab_size, paragraph_size,  embedding_dim, context_words_size)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "\n",
    "for epoch in range(epoch):\n",
    "    print(epoch)\n",
    "    \n",
    "    start_epoch = time.time()\n",
    "    total_loss=0\n",
    "    \n",
    "    \n",
    "    for batch in range(batch_size,len(product_desc_sample),batch_size):\n",
    "        start_batch = time.time()\n",
    "        \n",
    "        #filter dataframe for batches\n",
    "        product_desc_batch = product_desc_sample[batch-batch_size:batch]\n",
    "\n",
    "        #tokenize words\n",
    "        tokenized_words = tokenize_words(words_dict,product_desc_batch)\n",
    "\n",
    "        #create target words\n",
    "        preprocessed_data, target_word = create_target_words(tokenized_words,paragraph_dict)\n",
    "        \n",
    "        context = torch.tensor(preprocessed_data, dtype=torch.long)\n",
    "        \n",
    "        #reset tensor to zero for each epoch\n",
    "        model.zero_grad()\n",
    "        \n",
    "        #calculate probability of target word\n",
    "        log_probs = model(context)\n",
    "\n",
    "        #calculate loss using probability compared to actual\n",
    "        loss = loss_function(log_probs, torch.tensor(target_word,dtype=torch.long))\n",
    "    \n",
    "        #Run backward propogation\n",
    "        loss.backward()\n",
    "\n",
    "        #take step in gradient descent \n",
    "        optimizer.step()                                                                           \n",
    "\n",
    "        #total loss for epoch\n",
    "        total_loss += loss.item()   \n",
    "\n",
    "        #update total loss                                                               \n",
    "        losses.append(total_loss)\n",
    "\n",
    "    end_epoch = time.time()\n",
    "\n",
    "print(losses)  # The loss decreased every iteration over the training data!\n",
    "\n",
    "model_end = time.time()\n",
    "\n",
    "print(model_end-model_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load model\n",
    "model.load_state_dict(torch.load(\"C:\\\\Users\\\\kyle.becker\\\\Desktop\\\\Word Embedding Results\\\\model V3\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get word embedding for each paragraph\n",
    "embeds = model.embedding\n",
    "\n",
    "#create list to hold embeddings for each paragraph\n",
    "paragraph_embeddings = []\n",
    "\n",
    "#loop through each paragraph and return embedding for each paragraph\n",
    "for i in range(0,len(paragraph_dict)):\n",
    "    tokenized_word_embedding = torch.tensor(paragraph_dict['paragraph_'+str(i)], dtype=torch.long)\n",
    "    word_embedding = embeds(tokenized_word_embedding)\n",
    "    word_embeddings_array = word_embedding.detach().numpy().reshape(1,-1)[0]\n",
    "    word_embeddings_list = word_embeddings_array.tolist()\n",
    "    paragraph_embeddings.append(word_embeddings_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save embeddings of paragraphs\n",
    "import pickle\n",
    "\n",
    "with open('C:\\\\Users\\\\kyle.becker\\\\Desktop\\\\Word Embedding Results.txt, \"wb\") as fp:\n",
    "        pickle.dump(paragraph_embeddings, fp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
