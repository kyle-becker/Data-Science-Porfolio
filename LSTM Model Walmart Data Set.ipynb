{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import data\n",
    "features = pd.read_csv(\"C:\\\\Users\\\\kyle.becker\\\\Desktop\\\\Walmart LSTM\\\\features.csv\")\n",
    "stores = pd.read_csv(\"C:\\\\Users\\\\kyle.becker\\\\Desktop\\\\Walmart LSTM\\\\stores.csv\")\n",
    "train = pd.read_csv(\"C:\\\\Users\\\\kyle.becker\\\\Desktop\\\\Walmart LSTM\\\\train.csv\")\n",
    "test = pd.read_csv(\"C:\\\\Users\\\\kyle.becker\\\\Desktop\\\\Walmart LSTM\\\\test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(421570, 5)\n",
      "(421570, 17)\n"
     ]
    }
   ],
   "source": [
    "#join dataframes together to get training set\n",
    "train_features = pd.merge(left=train, right=features, on = ['Store', 'Date'])\n",
    "train_features_stores = pd.merge(left=train_features,right=stores, on = ['Store'])\n",
    "\n",
    "#Check rows before and after\n",
    "print(train.shape)\n",
    "print(train_features_stores.shape) \n",
    "\n",
    "#rename \n",
    "train_df = train_features_stores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(115064, 4)\n",
      "(115064, 16)\n"
     ]
    }
   ],
   "source": [
    "#join dataframes together for test set\n",
    "test_features = pd.merge(left=test,right=features, on = ['Store', 'Date'])\n",
    "test_features_stores = pd.merge(left=test_features, right=stores, on = ['Store'])\n",
    "\n",
    "#check rows before and after\n",
    "print(test.shape)\n",
    "print(test_features_stores.shape)\n",
    "\n",
    "test_df = test_features_stores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Weekly_Sales']\n"
     ]
    }
   ],
   "source": [
    "#see column that does not exist in test data set\n",
    "missing_column = [train_df.columns[i] for i in range(0,len(train_df.columns)) if train_df.columns[i] not in test_df.columns]\n",
    "print(missing_column)\n",
    "\n",
    "#column is missing weekly sales because the results must be submitted to be scored by the contest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#order by date and store\n",
    "train_df_sorted = train_df.sort_values(by=['Store', 'Dept', 'Date'])\n",
    "#shift values so prior sales can be used to predict future\n",
    "train_df_sorted['Weekly_Sales'] = train_df_sorted.groupby(['Store', 'Dept'])['Weekly_Sales'].shift(1)\n",
    "train_df_sorted['Temperature'] = train_df_sorted.groupby(['Store', 'Dept'])['Temperature'].shift(1)\n",
    "train_df_sorted['Fuel_Price'] = train_df_sorted.groupby(['Store', 'Dept'])['Fuel_Price'].shift(1)\n",
    "train_df_sorted['CPI'] = train_df_sorted.groupby(['Store', 'Dept'])['CPI'].shift(1)\n",
    "train_df_sorted['Unemployment'] = train_df_sorted.groupby(['Store', 'Dept'])['Unemployment'].shift(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter to remove na\n",
    "train_df_sorted = train_df_sorted[train_df_sorted['Weekly_Sales'].notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#select key columns and rename duplicate column\n",
    "train_for_model = train_df_sorted.loc[:,['Date','Store','Dept','IsHoliday_x', 'Temperature', 'Fuel_Price', 'MarkDown1', 'MarkDown2',\n",
    "                                       'MarkDown3', 'MarkDown4', 'MarkDown5', 'CPI', 'Unemployment', \n",
    "                                        'Weekly_Sales']].rename(columns={'IsHoliday_x':'IsHoliday'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fill in 0\n",
    "train_for_model = train_for_model.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert IsHoliday to binary\n",
    "train_for_model = pd.get_dummies(train_for_model, columns=['IsHoliday'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Temperature', 'Fuel_Price', 'MarkDown1', 'MarkDown2', 'MarkDown3',\n",
      "       'MarkDown4', 'MarkDown5', 'CPI', 'Unemployment', 'Weekly_Sales',\n",
      "       'IsHoliday_False', 'IsHoliday_True'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "#split data to normalize\n",
    "train_for_model_identifiers = train_for_model.loc[:,['Date', 'Store', 'Dept']].reset_index().drop(columns='index')\n",
    "train_for_model_features = train_for_model.drop(columns = ['Date', 'Store', 'Dept']).reset_index().drop(columns='index')\n",
    "print(train_for_model_features.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kyle.becker\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:323: DataConversionWarning: Data with input dtype uint8, float64 were all converted to float64 by MinMaxScaler.\n",
      "  return self.partial_fit(X, y)\n"
     ]
    }
   ],
   "source": [
    "#normalize data\n",
    "normalize = MinMaxScaler(feature_range=(0,1))\n",
    "features_scaled = normalize.fit_transform(train_for_model_features)\n",
    "\n",
    "#normalize y\n",
    "output_scaled = normalize.fit_transform(train_for_model_features['Weekly_Sales'].values.reshape(-1,1))\n",
    "\n",
    "df_features_scaled = pd.DataFrame(features_scaled, columns = train_for_model_features.columns).reset_index()\n",
    "#Y_scaled = normalize.fit_transform(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Join back \n",
    "df_train = pd.concat([train_for_model_identifiers, df_features_scaled], axis=1, sort=False)\n",
    "df_train = df_train.drop(columns=['index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lstm_groupings(df,lookback):\n",
    "    #blank list to hold lagged sales for lstm\n",
    "    groupings_df = []\n",
    "    #sort data by store dept and date to find\n",
    "    #previous sales lag\n",
    "    df = df.sort_values(by=['Store', 'Dept', 'Date'])\n",
    "    #convert df to list\n",
    "    X_list = df.values.tolist()\n",
    "    for row in range(2,len(X_list)):\n",
    "        current_row = X_list[row]\n",
    "        grouped_list = [current_row]\n",
    "        for lookback in range(1,lookback+1):\n",
    "            #find sales records that occur\n",
    "            #specified lags before record\n",
    "            previous_row = [X_list[row-lookback]]\n",
    "            \n",
    "            if previous_row[0][1]==grouped_list[0][1] and previous_row[0][2]==grouped_list[0][2]:\n",
    "                #if the lagged records have the same store and department add to list\n",
    "                grouped_list = grouped_list+previous_row\n",
    "        if len(grouped_list)==lookback:\n",
    "            #if user defined lag of records is found append to list\n",
    "            groupings_df.append(grouped_list)\n",
    "    \n",
    "    return groupings_df\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run user defined function to create groups of lagged sales\n",
    "X_groupings  = create_lstm_groupings(df_train,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove identifying information (store, department) to pass to model\n",
    "def remove_identifiers(X_list):\n",
    "    X_scaled = []\n",
    "    Y_scaled = []\n",
    "    for i in range(0,len(X_list)):\n",
    "        record = []\n",
    "        Y_actual = X_list[i][0][12]\n",
    "        Y_scaled.append(Y_actual)\n",
    "        for j in range(0,len(X_list[i])):\n",
    "            record = record+[X_list[i][j][3:12]+X_list[i][j][13:15]]\n",
    "        X_scaled.append(record)\n",
    "    return X_scaled, Y_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run user defined function to remove identifying information\n",
    "X_scaled, Y_scaled = remove_identifiers(X_groupings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert list to array as this is the format lstm \n",
    "#takes data in\n",
    "def add_results_to_array(X_scaled):\n",
    "    X_scaled_array = np.empty((0,3,11), float)\n",
    "    for i in range(0,len(X_scaled)):\n",
    "        if  i%10000==0:\n",
    "            print(i)\n",
    "        row = X_scaled[i]\n",
    "        X_scaled_array = np.append(X_scaled_array, [row], axis=0)\n",
    "    return X_scaled_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run user defined function to convert list to array\n",
    "array_lstm = add_results_to_array(X_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save results to pickle\n",
    "import pickle\n",
    "output = open('C:\\\\Users\\\\kyle.becker\\\\Desktop\\\\Walmart LSTM\\\\array_lstm.pkl', 'wb')\n",
    "pickle.dump(array_lstm, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in array\n",
    "import pickle\n",
    "infile = open('C:\\\\Users\\\\kyle.becker\\\\Desktop\\\\Walmart LSTM\\\\array_lstm.pkl','rb')\n",
    "array_lstm = pickle.load(infile)\n",
    "array_lstm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split data set variables into test and train\n",
    "import random\n",
    "\n",
    "train_sample = np.random.choice(array_lstm.shape[0], 300000, replace=False)\n",
    "list_train_sample = list(train_sample)\n",
    "\n",
    "all_sample =[i for i in range(0,array_lstm.shape[0])]\n",
    "\n",
    "test_sample = [i for i in range(0,array_lstm.shape[0]) if i not in train_sample]\n",
    "\n",
    "train_X = array_lstm[train_sample, :]\n",
    "test_X = array_lstm[test_sample, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split output variable into train and test\n",
    "Y = np.array([Y_scaled]).reshape(-1,1)\n",
    "\n",
    "train_Y = Y[train_sample,:]\n",
    "test_Y = Y[test_sample,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create lstm model\n",
    "model = Sequential()\n",
    "model.add(LSTM(units=10, activation='tanh', input_shape=(3,11)))\n",
    "model.add(Dense(units=1))\n",
    "#use adam optimization and mean squared error to measure results\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit model\n",
    "model.fit(train_X,train_Y, epochs=10, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "394260776.36218774\n",
      "394242666.32123613\n"
     ]
    }
   ],
   "source": [
    "#reverse normalization\n",
    "Y_train_pred = model.predict(train_X)\n",
    "Y_train_pred = normalize.inverse_transform(Y_train_pred)\n",
    "#calculate training error\n",
    "training_error = mean_squared_error(train_Y,Y_train_pred)\n",
    "#print training error\n",
    "print(training_error)\n",
    "\n",
    "#reverse normalization\n",
    "Y_test_pred = model.predict(test_X)\n",
    "Y_test_pred = normalize.inverse_transform(Y_test_pred)\n",
    "#calculate test error\n",
    "testing_error = mean_squared_error(test_Y,Y_test_pred)\n",
    "#print test error\n",
    "print(testing_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model is experiencing large bias need to increase the number of epochs to decrease bias\n",
    "# and increase the learning rate"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
